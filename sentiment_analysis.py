# -*- coding: utf-8 -*-
"""Sentiment Analysis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1I0HRLbdhtknGetOzXhRGwFDuPaayDvfK
"""

import numpy as np
import pandas as pd
import seaborn as sns
import re
from nltk.corpus import stopwords
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS
# Text Normalization: Stemming or Lemmatization (prefer)
from nltk.stem import WordNetLemmatizer
import contractions
import matplotlib.pyplot as plt
from sklearn.metrics import accuracy_score
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import cross_val_score
from sklearn import svm
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import RandomizedSearchCV
from sklearn.metrics import confusion_matrix

data_set = pd.read_csv("/content/testdata.manual.2009.06.14.csv")
data_set.columns = ['sentiment','id','date','query','user','tweet_text']

pd.DataFrame(data_set)

data_set.info()

data_set.shape

data_set.describe()

data_set["sentiment"].value_counts()

data_set["sentiment"].value_counts(normalize = True)

data_set["id"].value_counts()

data_set.isnull().sum()

data_set["sentiment"].value_counts().plot(kind = "bar")

import nltk
nltk.download('stopwords')
nltk_stopwords = set(stopwords.words('english'))
print(nltk_stopwords)

len(nltk_stopwords)

sklearn_stopwords = set(ENGLISH_STOP_WORDS)
print(sklearn_stopwords)

len(sklearn_stopwords)

# Find the common stopwords from NLTK & sklearn
print(nltk_stopwords.intersection(sklearn_stopwords))

len(nltk_stopwords.intersection(sklearn_stopwords))

# Combining the stopwords from sklearn & NLTK
combined_stopwords = nltk_stopwords.union(sklearn_stopwords)

len(combined_stopwords)

nltk.download('wordnet')

# Text Normalization: Stemming or Lemmatization (prefer
lemmatizer = WordNetLemmatizer()

"""Define the Cleaner Function & Apply

"""

def tweet_cleaner_without_stopwords(text):
    new_text = re.sub(r"'s\b", " is", text)
    new_text = re.sub("#", "", new_text)
    new_text = re.sub("@[A-Za-z0-9]+", "", new_text)
    new_text = re.sub(r"http\S+", "", new_text)
    new_text = contractions.fix(new_text)
    new_text = re.sub(r"[^a-zA-Z]", " ", new_text)
    new_text = new_text.lower().strip()

    cleaned_text = ''
    for token in new_text.split():
        cleaned_text = cleaned_text + lemmatizer.lemmatize(token) + ' '

    return cleaned_text

cleaned_tweets = []  # list of cleaned tweets
for twt in data_set['tweet_text']:
    cleaned_tweets.append(tweet_cleaner_without_stopwords(twt))

print(data_set["tweet_text"][:5])

cleaned_tweets[:5]

print(data_set["tweet_text"][24])

cleaned_tweets[24]

data_set['cleaned_tweets_w/o_SW'] = cleaned_tweets
data_set.head()

"""3. Data Visualization

3. Data Visualization
Plot the top 25 most common words in this tweets dataset
"""

# Write a code to collect all the words from all the tweets into a single list
all_words = []
for t in data_set['tweet_text']:
    all_words.extend(t.split())

print(all_words[:50])
len(set(all_words)) # this is the number of unique words in the list

# Frequency Distribution
freq_dist = nltk.FreqDist(all_words)

plt.figure(figsize=(12,5))
plt.title('Top 25 most common words')
plt.xticks(fontsize=15)

freq_dist.plot(25, cumulative=False)

plt.show()

"""Plot for Cleaned Tweets"""

all_words = []
for t in data_set['cleaned_tweets_w/o_SW']:
    all_words.extend(t.split())

print(all_words[:50])

len(set(all_words)) # this is the number of unique words in the list

# Frequency Distribution
freq_dist = nltk.FreqDist(all_words)

plt.figure(figsize=(12,5))
plt.title('Top 25 most common words')
plt.xticks(fontsize=15)

freq_dist.plot(25, cumulative=False)

plt.show()

"""Data Visualization After applying Stop words"""

def tweet_cleaner_with_stopwords(text):
    new_text = re.sub(r"'s\b", " is", text)
    new_text = re.sub("#", "", new_text)
    new_text = re.sub("@[A-Za-z0-9]+", "", new_text)
    new_text = re.sub(r"http\S+", "", new_text)
    new_text = contractions.fix(new_text)
    new_text = re.sub(r"[^a-zA-Z]", " ", new_text)
    new_text = new_text.lower().strip()

    new_text = [token for token in new_text.split() if token not in combined_stopwords]

    new_text = [token for token in new_text if len(token)>2]

    cleaned_text = ''
    for token in new_text:
        cleaned_text = cleaned_text + lemmatizer.lemmatize(token) + ' '

    return cleaned_text

cleaned_tweets = list(data_set['tweet_text'].apply(tweet_cleaner_with_stopwords))
print(cleaned_tweets[:10])

data_set.columns

data_set['cleaned_tweets_with_SW'] = cleaned_tweets
data_set.head()

all_words = []
for t in data_set['cleaned_tweets_with_SW']:
    all_words.extend(t.split())

print(all_words[:50])

# Frequency Distribution
freq_dist = nltk.FreqDist(all_words)

plt.figure(figsize=(12,5))
plt.title('Top 25 most common words')
plt.xticks(fontsize=15)

freq_dist.plot(25, cumulative=False)

plt.show()

data_set.head()

"""4. Bag of Words Model (Feature Extraction)

TF-IDF Vectorizer
"""

X = data_set["cleaned_tweets_with_SW"]
Y = data_set["sentiment"]

# converting the textual data to numerical data
vectorizer = TfidfVectorizer()
vectorizer.fit(X)

X = vectorizer.transform(X)

print(X)

X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.25,random_state = 3)

model = svm.SVC(kernel='linear')
model.fit(X_train, Y_train)

print(model.score(X_train, Y_train))  # train score)
print(model.score(X_test, Y_test))

"""Cross Validation"""

model.fit(X_train, Y_train)

# evaluating the model
test_data_prediction = model.predict(X_test)

accuracy = accuracy_score(Y_test, test_data_prediction)

print('Accuracy score of the ', model, ' = ', accuracy)

cv_score_svc = cross_val_score(SVC(kernel='linear'), X, Y, cv=5)

print(cv_score_svc)

mean_accuracy_svc = sum(cv_score_svc)/len(cv_score_svc)

mean_accuracy_svc = mean_accuracy_svc*100

mean_accuracy_svc = round(mean_accuracy_svc, 2)

print(mean_accuracy_svc)

"""Hyperparameter Tuning:

GridSearchCV
RandomizedSearchCV
"""

# hyperparameters

parameters = {
              'kernel':['linear','poly','rbf','sigmoid'],
              'C':[1, 5, 10, 20]
}

# grid search
classifier1 = GridSearchCV(model, parameters, cv=5)

# fitting the data to our model
classifier1.fit(X, Y)

classifier1.cv_results_

# best parameters

best_parameters = classifier1.best_params_
print(best_parameters)

# higest accuracy

highest_accuracy = classifier1.best_score_
print(highest_accuracy)

# loading the results to pandas dataframe
result = pd.DataFrame(classifier1.cv_results_)

result.head()

grid_search_result = result[['param_C','param_kernel','mean_test_score']]

grid_search_result

"""RandomizedSearchCV"""

# grid search
classifier2 = RandomizedSearchCV(model, parameters, cv=5)

# fitting the data to our model
classifier2.fit(X, Y)

# best parameters

best_parameters = classifier2.best_params_
print(best_parameters)

# With respect to GridSearchCV
model = svm.SVC(C=1, kernel= 'linear')
model.fit(X_train, Y_train)

print(model.score(X_train, Y_train))  # train score)
print(model.score(X_test, Y_test))

# With respect to RandomGridSearchCV
model = svm.SVC(C=10, kernel= 'rbf')
model.fit(X_train, Y_train)

print(model.score(X_train, Y_train))  # train score)
print(model.score(X_test, Y_test))